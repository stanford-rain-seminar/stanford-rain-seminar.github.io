- type: "Upcoming Talks"
  members:
    - speaker: "Adam Kalai"
      date: "Nov 18, 2025"
      title: "Why Language Models Hallucinate"
      abstract: |
        Large language models (LLMs) sometimes generate statements that are plausible but factually incorrect—a phenomenon commonly called "hallucination." We argue that these errors are not mysterious failures of architecture or reasoning, but rather predictable consequences of standard training and evaluation incentives. We show (i) that hallucinations can be viewed as classification errors: when pretrained models cannot reliably distinguish a false statement from a true one, they may produce the false option rather than saying I don't know; (ii) that optimization of benchmark performance encourages guessing rather than abstaining, since most evaluation metrics penalize expressing uncertainty; and (iii) that a possible mitigation path lies in revising existing benchmarks to reward calibrated abstention, thus realigning incentives in model development. Joint work with Santosh Vempala (Georgia Tech) and Ofir Nachum & Edwin Zhang (OpenAI).
      bio: |
        Adam Tauman Kalai is a Research Scientist at OpenAI, specializing in AI Safety and Ethics. His research interests also include algorithms, AI theory, and game theory. Adam earned his BA from Harvard University and his PhD from Carnegie Mellon University, after which he served as an Assistant Professor at TTIC and Georgia Tech and a Senior Principal Researcher at Microsoft Research New England. He is also a member of Project CETI's science team.
    - speaker: "Negin Golrezaei"
      date: "Dec 2, 2025"
- type: "Previous Talks This Year"
  members:
    - speaker: "Sarah Cen"
      date: "Nov 11, 2025"
      title: "Bridging the Gap Between Research and Policy in AI Safety and Accountability"
      abstract: |
        As AI becomes increasingly integrated into both the private and public sectors, challenges around AI safety and policy have arisen. There is a growing, compelling body of work around the legal and societal challenges that come with AI, but there is a gap in our rigorous understanding of these problems. In this talk, I dive deep into a few topics in AI safety and policy. We will discuss AI supply chains (the increasingly complex ecosystem of AI actors and components that contribute to AI products) and study how AI supply chains complicate machine learning objectives. We'll then shift our discussion to AI audits and evidentiary burdens in cases involving AI. Using Pareto frontiers as a tool for assessing performance-fairness tradeoffs, we will show how a closed-form expression for performance-fairness Pareto frontiers can help plaintiffs (or auditors) overcome evidentiary burdens or a lack of access in AI contexts. I'll conclude with a longitudinal study of LLMs during the 2024 US election season. If time permits, we may touch on formal notions of trustworthiness.
      bio: |
        Sarah Cen is a postdoc at Stanford University and incoming Assistant Professor at Carnegie Mellon University's Departments of ECE & EPP. At Stanford, Sarah works with Prof. Percy Liang in Computer Science and Prof. Daniel Ho in the Stanford Law School. Her research is interdisciplinary and inspired by works in machine learning, economics, law, and policy. She has ongoing work on algorithmic auditing, AI supply chains, due process for AI determinations, risk under the EU AI Act, and formalizing trustworthy algorithms. Previously, Sarah received her BSE in Mechanical Engineering from Princeton University and Master's in Engineering Science (Robotics) from Oxford University, where she worked on autonomous vehicles.
    - speaker: "Dylan Hadfield-Menell"
      date: "Nov 4, 2025"
      title: "Building aligned agents for open-universe tasks"
      abstract: |
        As agents move from the lab into real-world settings, designers have a limited ability to anticipate the agent's context and design explicit safeguards. In this talk, I will outline challenges that this raises from the perspective of designing flexible, robust, and aligned agent behaviors. The key to the approach is to design agents that can model and respond to appropriate uncertainty about a user's intended goal and the normative environment they are deployed into. I will begin with a survey of current alignment techniques and AI agents, then outline the theoretical motivation for this approach. Next, I will describe recent work from my lab that attempts to address this problem by 1) designing flexible goal inference mechanisms that can track the set of plausible user goals reliably from context; and 2) integrating these inference tools with efficient agent designs that leverage POMDP solvers in order to train agents that implement belief-constrained behaviors. I will conclude with a discussion of recent work that evaluates collaborative agents and discuss the implications for the design of aligned systems that augment and integrate with human users and intent.
      bio: |
        Dylan Hadfield-Menell is an Associate Professor of EECS at MIT. His research develops methods to ensure that AI systems behavior aligns with the goals and values of their human users and society as a whole, a concept known as 'AI alignment'. His goal is to enable the safe, beneficial, and trustworthy deployment of AI in real-world settings.
    - speaker: "Connor Lawless"
      date: "Oct 21, 2025"
      title: "Democratizing Optimization via Generative AI"
      abstract: |
        From healthcare delivery to resilient power grid management, optimization has the potential to improve decision-making for some of today’s most pressing problems, but its use is often limited by the mathematical expertise required to model and solve complex problems. This talk will showcase the potential of generative AI to lower this barrier and democratize access to advanced optimization tools. Motivated by a collaboration with Microsoft Outlook, the first part of the talk will present a novel framework for interactive decision support for non-expert users that leverages large language models (LLM) to translate user requests into an underlying constraint programming model. We investigate this framework through the lens of meeting scheduling, and showcase its potential via a user study with a prototype system. In the second part of the talk, we demonstrate how LLMs can be used to automatically generate problem-specific optimization solver configurations, a challenging task for even expert optimization users. Our approach achieves up to 70% speed-ups over default solver settings with little-to-no additional compute. We will conclude by discussing broader opportunities for integrating natural language and optimization, moving toward a future where powerful decision-making tools are as accessible for managers at a local food bank as they are for applied scientists at Amazon.
      bio: |
        Connor Lawless is a Postdoctoral Fellow at the Stanford Institute for Human- Centered Artificial Intelligence advised by Ellen Vitercik and Madeleine Udell. His research blends tools from optimization, machine learning, and human-computer interaction to make advanced analytics tools more accessible and trustworthy. He received his PhD in Operations Research from Cornell University where he was advised by Oktay Gunluk, and previously spent time at Microsoft Research, IBM Research, and the Royal Bank of Canada.
    - speaker: "Alice Oh"
      date: "Oct 14, 2025"
      title: "LLM Evaluation for the Real World"
      abstract: | 
        Traditional evaluation methods for large language models (LLMs)—often centered on accuracy in static multiple-choice or short-answer questions—fail to capture real-world complexities. As LLMs increasingly serve users in dynamic, multicultural contexts, we must redefine meaningful evaluation. This talk presents our recent research advancing LLM evaluation through culturally aware, socially grounded, and customizable benchmarks. We assess factual consistency across languages, everyday knowledge in underrepresented cultures, and cultural inclusivity. We highlight that biases become evident in generation tasks, reflecting actual LLM use. Central to our approach is BenchHub, a unified benchmark suite categorizing over 300,000 questions across diverse domains and cultures, enabling tailored evaluations. BenchHub underscores domain-specific variations and the critical role benchmark composition plays in LLM performance rankings. These insights demonstrate that accuracy alone is insufficient; comprehensive LLM evaluation must consider culture, context, and customization. This talk advocates a broader evaluation agenda, presenting foundational steps toward robust, inclusive assessments.
      bio: |
        Alice Oh is a Professor in the School of Computing at KAIST. Her major research area is at the intersection of natural language processing (NLP) and computational social science, with a recent focus on multilingual and multicultural aspects of LLMs. She collaborates with scholars in humanities and social sciences such as political science, education, and history. She has served as Program Chair for ICLR 2021 and NeurIPS 2022, General Chair for ACM FAccT 2022 and NeurIPS 2023, and DEI Chair for COLM 2024. She is the current President of SIGDAT which oversees EMNLP.
    - speaker: "Nika Haghtalab"
      date: "Oct 7, 2025"
      title: "Distortion of Learning and AI Alignment from Heterogenous Human Feedback"
      abstract: |
        After pre-training, large language models are aligned with human preferences based on crowdsourced pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users in any meaningful way.  In this work, we ask a deceptively simple yet foundational question: Do state-of-the-art alignment methods actually produce models that satisfy users on average in the presence of heterogeneous preferences? Drawing on social choice theory, and modeling each user’s comparisons via an individual Bradley–Terry (BT) model, we introduce the distortion of an alignment method: the worst-case ratio between the optimal achievable average utility and the average utility of the learned policy. This notion yields concrete insights into alignment with heterogeneous preferences. In particular, we establish an impossibility result for aligning to average user utility — counter to the conventional wisdom that ML methods, even if imperfect for every individual, at least perform well on average. Distortion also highlights sharp differences between alignment methods: we show that widely used approaches such as RLHF and DPO can have exponentially large — or even unbounded — distortion, whereas a constant minimax-optimal distortion is achievable via a method inspired by social choice theory, known as maximal lotteries, or Nash Learning from Human Feedback.
      bio: |
        Nika Haghtalab is an Assistant Professor in the Department of Electrical Engineering and Computer Sciences at UC Berkeley. She works on a broad and versatile set of problems related to machine learning, algorithms, economics, and society. Her work contributes to an emerging mathematical foundation for learning and decision-making systems in the presence of economic and societal forces. Her work has been recognized by a Sloan fellowship (2024), Schmidt Sciences AI2050 award, NSF CAREER (2022), Google Research Scholar award (2023), NeurIPS and ICAPS best paper awards, EC exemplary track paper awards, and several other industry awards and fellowships.
    - speaker: "Andrew Ilyas"
      date: "Sep 30, 2025"
      title: "Data Attribution, Selection, and Valuation at Scale with Metagradients"
      abstract: |
        Training data is now recognized as a key driver the performance of AI systems. Indeed, AI companies are signing multi-million dollar deals for training data acquisition, raising the question: how "should" this training data be priced? Understanding how to value training data requires us to understand the downstream impact of this data on model behavior---which is made challenging by the complex, uninterpretable nature of large-scale ML models.
        In the first part of this talk, we present some recent work on tracing back model performance to training data---improving on a long line of prior work in machine learning, our method can optimally (in a natural sense) predict the impact of training data on model performance. In the second part of the talk, we propose a framework for studying data pricing theoretically, inspired by our experimental results in the first part of the talk. We conclude with some open questions and directions.
      bio: |
        Andrew is an incoming Assistant Professor at CMU. Previously, he was a Stein Fellow at Stanford and a PhD student at MIT, where he was supported by an Open Philanthropy AI Fellowship. His interests are currently in understanding and predicting the effects of design choices on downstream machine learning systems.

